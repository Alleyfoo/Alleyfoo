{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "bbc02865",
   "metadata": {},
   "source": [
    "# Simulated Data Pipeline Playbook\n",
    "\n",
    "This notebook demonstrates how to simulate end-to-end pipelines that move data from an on-premises environment into a data lake and onward to Snowflake.\n",
    "We will focus on showing the **techniques** that practitioners would use, including how to reconcile schema differences before merging data into a final analytics database."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4268c826",
   "metadata": {},
   "source": [
    "## 0. Imports & Configuration\n",
    "\n",
    "We rely on pandas to stand in for the compute engines (Spark, Snowflake) and use Python dictionaries to emulate configuration artifacts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2905bae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory prepared: simulated_storage\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "BASE_PATH = Path('simulated_storage')\n",
    "BASE_PATH.mkdir(exist_ok=True)\n",
    "print('Working directory prepared:', BASE_PATH)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed1e2c39",
   "metadata": {},
   "source": [
    "## 1. Scenario Setup\n",
    "\n",
    "To keep a consistent business story, we will follow a fictional retail company consolidating online orders. Data originates from multiple source systems:\n",
    "\n",
    "* **AVS (On-Prem / VM)**: Legacy ERP extracts that use snake_case headers.\n",
    "* **Data Lake Landing**: Semi-structured JSON exports with camelCase headers and nested attributes.\n",
    "* **Snowflake**: Target warehouse expecting business-friendly Pascal Case headers.\n",
    "\n",
    "In real life we would connect to each platform with vendor-specific connectors. Here, we simulate those reads with local files while emphasizing best practices such as configuration-driven ingestion and schema validation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "302b94c9",
   "metadata": {},
   "source": [
    "### 1.1 Sample Source Files\n",
    "\n",
    "We create three mock datasets with intentionally mismatched headers to illustrate transformation needs:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc1d996e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   order_id customer_id  order_total             order_ts\n",
       " 0       101        C001       250.00  2024-03-01 10:15:00\n",
       " 1       102        C002       190.50  2024-03-01 12:30:00\n",
       " 2       103        C003       330.25  2024-03-02 09:45:00,\n",
       "   OrderID CustomerCode  GrossAmount             UpdatedAt\n",
       " 0   A-900         C002        210.0  2024-03-02T14:00:00Z\n",
       " 1   A-901         C004        480.0  2024-03-02T16:20:00Z,\n",
       "    ORDER NUMBER CUSTOMER  TOTAL $             MODIFIED\n",
       " 0          5001     C001   125.50  2024/03/03 08:30:00\n",
       " 1          5002     C005   275.75  2024/03/03 09:10:00)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "avs_orders = pd.DataFrame({\n",
    "    'order_id': [101, 102, 103],\n",
    "    'customer_id': ['C001', 'C002', 'C003'],\n",
    "    'order_total': [250.0, 190.5, 330.25],\n",
    "    'order_ts': ['2024-03-01 10:15:00', '2024-03-01 12:30:00', '2024-03-02 09:45:00']\n",
    "})\n",
    "\n",
    "raw_lake_orders = pd.DataFrame({\n",
    "    'OrderID': ['A-900', 'A-901'],\n",
    "    'CustomerCode': ['C002', 'C004'],\n",
    "    'GrossAmount': [210.0, 480.0],\n",
    "    'UpdatedAt': ['2024-03-02T14:00:00Z', '2024-03-02T16:20:00Z']\n",
    "})\n",
    "\n",
    "partner_feed = pd.DataFrame({\n",
    "    'ORDER NUMBER': [5001, 5002],\n",
    "    'CUSTOMER': ['C001', 'C005'],\n",
    "    'TOTAL $': [125.5, 275.75],\n",
    "    'MODIFIED': ['2024/03/03 08:30:00', '2024/03/03 09:10:00']\n",
    "})\n",
    "\n",
    "avs_orders, raw_lake_orders, partner_feed"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8568c357",
   "metadata": {},
   "source": [
    "## 2. Ingestion Techniques\n",
    "\n",
    "### 2.1 Configuration-Driven Connectors\n",
    "\n",
    "Real pipelines rely on configuration maps to determine connection parameters and mapping logic. Below we define metadata that describes each source, its storage layer, and how headers should be normalized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e91cb6d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avs_orders': {'layer': 'AVS',\n",
       "  'read_format': 'csv',\n",
       "  'primary_key': 'order_id',\n",
       "  'expected_headers': ['order_id', 'customer_id', 'order_total', 'order_ts'],\n",
       "  'rename_map': {'order_id': 'OrderID',\n",
       "   'customer_id': 'CustomerID',\n",
       "   'order_total': 'Amount',\n",
       "   'order_ts': 'UpdatedAt'}},\n",
       " 'raw_lake_orders': {'layer': 'DataLake',\n",
       "  'read_format': 'json',\n",
       "  'primary_key': 'OrderID',\n",
       "  'expected_headers': ['OrderID', 'CustomerCode', 'GrossAmount', 'UpdatedAt'],\n",
       "  'rename_map': {'OrderID': 'OrderID',\n",
       "   'CustomerCode': 'CustomerID',\n",
       "   'GrossAmount': 'Amount',\n",
       "   'UpdatedAt': 'UpdatedAt'}},\n",
       " 'partner_feed': {'layer': 'PartnerFTP',\n",
       "  'read_format': 'xlsx',\n",
       "  'primary_key': 'ORDER NUMBER',\n",
       "  'expected_headers': ['ORDER NUMBER', 'CUSTOMER', 'TOTAL $', 'MODIFIED'],\n",
       "  'rename_map': {'ORDER NUMBER': 'OrderID',\n",
       "   'CUSTOMER': 'CustomerID',\n",
       "   'TOTAL $': 'Amount',\n",
       "   'MODIFIED': 'UpdatedAt'}}}"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_config = {\n",
    "    'avs_orders': {\n",
    "        'layer': 'AVS',\n",
    "        'read_format': 'csv',\n",
    "        'primary_key': 'order_id',\n",
    "        'expected_headers': ['order_id', 'customer_id', 'order_total', 'order_ts'],\n",
    "        'rename_map': {\n",
    "            'order_id': 'OrderID',\n",
    "            'customer_id': 'CustomerID',\n",
    "            'order_total': 'Amount',\n",
    "            'order_ts': 'UpdatedAt'\n",
    "        }\n",
    "    },\n",
    "    'raw_lake_orders': {\n",
    "        'layer': 'DataLake',\n",
    "        'read_format': 'json',\n",
    "        'primary_key': 'OrderID',\n",
    "        'expected_headers': ['OrderID', 'CustomerCode', 'GrossAmount', 'UpdatedAt'],\n",
    "        'rename_map': {\n",
    "            'OrderID': 'OrderID',\n",
    "            'CustomerCode': 'CustomerID',\n",
    "            'GrossAmount': 'Amount',\n",
    "            'UpdatedAt': 'UpdatedAt'\n",
    "        }\n",
    "    },\n",
    "    'partner_feed': {\n",
    "        'layer': 'PartnerFTP',\n",
    "        'read_format': 'xlsx',\n",
    "        'primary_key': 'ORDER NUMBER',\n",
    "        'expected_headers': ['ORDER NUMBER', 'CUSTOMER', 'TOTAL $', 'MODIFIED'],\n",
    "        'rename_map': {\n",
    "            'ORDER NUMBER': 'OrderID',\n",
    "            'CUSTOMER': 'CustomerID',\n",
    "            'TOTAL $': 'Amount',\n",
    "            'MODIFIED': 'UpdatedAt'\n",
    "        }\n",
    "    }\n",
    "}\n",
    "source_config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "600a6045",
   "metadata": {},
   "source": [
    "### 2.2 Schema Validation Utility\n",
    "\n",
    "A lightweight validation function checks whether the incoming headers match expectations. In production, this step would raise alerts or move files to quarantine when mismatches occur."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "66b048cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] avs_orders: headers aligned\n",
      "[OK] raw_lake_orders: headers aligned\n",
      "[OK] partner_feed: headers aligned\n"
     ]
    }
   ],
   "source": [
    "def validate_headers(df: pd.DataFrame, config: dict, *, source: str) -> None:\n",
    "    actual = list(df.columns)\n",
    "    expected = config[source]['expected_headers']\n",
    "    missing = set(expected) - set(actual)\n",
    "    extra = set(actual) - set(expected)\n",
    "    if missing or extra:\n",
    "        print(f\"[WARN] {source}: header mismatch detected\")\n",
    "        if missing:\n",
    "            print('  Missing columns:', ', '.join(sorted(missing)))\n",
    "        if extra:\n",
    "            print('  Unexpected columns:', ', '.join(sorted(extra)))\n",
    "    else:\n",
    "        print(f\"[OK] {source}: headers aligned\")\n",
    "\n",
    "validate_headers(avs_orders, source_config, source='avs_orders')\n",
    "validate_headers(raw_lake_orders, source_config, source='raw_lake_orders')\n",
    "validate_headers(partner_feed, source_config, source='partner_feed')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03afc27",
   "metadata": {},
   "source": [
    "## 3. Data Lake Landing (Bronze/Silver)\n",
    "\n",
    "We mimic a bronze-to-silver process. Bronze keeps the data as-is but tracks metadata. Silver standardizes header names, harmonizes datatypes, and enriches records with a unified surrogate key."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "57e582d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'avs_orders_silver':    OrderID CustomerID  Amount            UpdatedAt IngestedFrom\n",
       " 0      101       C001  250.00  2024-03-01 10:15:00          AVS\n",
       " 1      102       C002  190.50  2024-03-01 12:30:00          AVS\n",
       " 2      103       C003  330.25  2024-03-02 09:45:00          AVS,\n",
       " 'raw_lake_orders_silver':   OrderID CustomerID  Amount             UpdatedAt IngestedFrom\n",
       " 0   A-900       C002   210.0  2024-03-02T14:00:00Z     DataLake\n",
       " 1   A-901       C004   480.0  2024-03-02T16:20:00Z     DataLake,\n",
       " 'partner_feed_silver':    OrderID CustomerID  Amount            UpdatedAt IngestedFrom\n",
       " 0     5001       C001  125.50  2024/03/03 08:30:00   PartnerFTP\n",
       " 1     5002       C005  275.75  2024/03/03 09:10:00   PartnerFTP}"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def normalize_headers(df: pd.DataFrame, rename_map: dict) -> pd.DataFrame:\n",
    "    return df.rename(columns=rename_map)\n",
    "\n",
    "bronze_tables = {\n",
    "    'avs_orders_bronze': avs_orders.copy(),\n",
    "    'raw_lake_orders_bronze': raw_lake_orders.copy(),\n",
    "    'partner_feed_bronze': partner_feed.copy()\n",
    "}\n",
    "\n",
    "silver_tables = {}\n",
    "for source, bronze_df in bronze_tables.items():\n",
    "    cfg_key = source.replace('_bronze', '')\n",
    "    renamed = normalize_headers(bronze_df, source_config[cfg_key]['rename_map'])\n",
    "    renamed['IngestedFrom'] = source_config[cfg_key]['layer']\n",
    "    silver_tables[source.replace('_bronze', '_silver')] = renamed\n",
    "\n",
    "silver_tables"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af45dce",
   "metadata": {},
   "source": [
    "### 3.1 Type Harmonization\n",
    "\n",
    "Notice that timestamps and numeric fields follow different patterns. We centralize casting rules to avoid repeated transformation logic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "605d07f8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderID         string[python]\n",
       "CustomerID      string[python]\n",
       "Amount                 float64\n",
       "UpdatedAt       datetime64[ns]\n",
       "IngestedFrom            object\n",
       "dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtype_rules = {\n",
    "    'OrderID': 'string',\n",
    "    'CustomerID': 'string',\n",
    "    'Amount': 'float',\n",
    "    'UpdatedAt': 'datetime64[ns]'\n",
    "}\n",
    "\n",
    "for table_name, df in silver_tables.items():\n",
    "    typed_df = df.astype({k: v for k, v in dtype_rules.items() if k in df.columns}, errors='ignore')\n",
    "    typed_df['UpdatedAt'] = pd.to_datetime(typed_df['UpdatedAt'], errors='coerce')\n",
    "    silver_tables[table_name] = typed_df\n",
    "\n",
    "silver_tables['avs_orders_silver'].dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "360ccfa0",
   "metadata": {},
   "source": [
    "## 4. Snowflake Loading & Transformation\n",
    "\n",
    "In production we would use Snowflake stages and `COPY INTO` commands. Here, we consolidate the silver tables into a single **gold** table while applying business rules:\n",
    "\n",
    "* Deduplicate on `OrderID` preferring the most recent `UpdatedAt`.\n",
    "* Create human-readable headers expected by analytics teams.\n",
    "* Split the output into two presentation formats: a wide executive report and a transactional view."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c5feec19",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderID</th>\n",
       "      <th>CustomerID</th>\n",
       "      <th>Amount</th>\n",
       "      <th>UpdatedAt</th>\n",
       "      <th>IngestedFrom</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>C001</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2024-03-01 10:15:00+00:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>C002</td>\n",
       "      <td>190.50</td>\n",
       "      <td>2024-03-01 12:30:00+00:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>C003</td>\n",
       "      <td>330.25</td>\n",
       "      <td>2024-03-02 09:45:00+00:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-900</td>\n",
       "      <td>C002</td>\n",
       "      <td>210.00</td>\n",
       "      <td>2024-03-02 14:00:00+00:00</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-901</td>\n",
       "      <td>C004</td>\n",
       "      <td>480.00</td>\n",
       "      <td>2024-03-02 16:20:00+00:00</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5001</td>\n",
       "      <td>C001</td>\n",
       "      <td>125.50</td>\n",
       "      <td>2024-03-03 08:30:00+00:00</td>\n",
       "      <td>PartnerFTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5002</td>\n",
       "      <td>C005</td>\n",
       "      <td>275.75</td>\n",
       "      <td>2024-03-03 09:10:00+00:00</td>\n",
       "      <td>PartnerFTP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  OrderID CustomerID  Amount                 UpdatedAt IngestedFrom\n",
       "0     101       C001  250.00 2024-03-01 10:15:00+00:00          AVS\n",
       "1     102       C002  190.50 2024-03-01 12:30:00+00:00          AVS\n",
       "2     103       C003  330.25 2024-03-02 09:45:00+00:00          AVS\n",
       "3   A-900       C002  210.00 2024-03-02 14:00:00+00:00     DataLake\n",
       "4   A-901       C004  480.00 2024-03-02 16:20:00+00:00     DataLake\n",
       "5    5001       C001  125.50 2024-03-03 08:30:00+00:00   PartnerFTP\n",
       "6    5002       C005  275.75 2024-03-03 09:10:00+00:00   PartnerFTP"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "combined = pd.concat(silver_tables.values(), ignore_index=True)\n",
    "combined['UpdatedAt'] = pd.to_datetime(combined['UpdatedAt'], utc=True, errors='coerce')\n",
    "combined = combined.sort_values('UpdatedAt', na_position='first').drop_duplicates('OrderID', keep='last')\n",
    "combined['Amount'] = pd.to_numeric(combined['Amount'], errors='coerce').round(2)\n",
    "combined"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84a4d604",
   "metadata": {},
   "source": [
    "### 4.1 Final Warehouse Schema\n",
    "\n",
    "Snowflake tables often use Pascal Case. We also demonstrate how to produce variant header layouts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f3c5a455",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderId</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>NetAmount</th>\n",
       "      <th>UpdatedAtUtc</th>\n",
       "      <th>SourceSystem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>C001</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2024-03-01 10:15:00+00:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>C002</td>\n",
       "      <td>190.50</td>\n",
       "      <td>2024-03-01 12:30:00+00:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>C003</td>\n",
       "      <td>330.25</td>\n",
       "      <td>2024-03-02 09:45:00+00:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-900</td>\n",
       "      <td>C002</td>\n",
       "      <td>210.00</td>\n",
       "      <td>2024-03-02 14:00:00+00:00</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-901</td>\n",
       "      <td>C004</td>\n",
       "      <td>480.00</td>\n",
       "      <td>2024-03-02 16:20:00+00:00</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5001</td>\n",
       "      <td>C001</td>\n",
       "      <td>125.50</td>\n",
       "      <td>2024-03-03 08:30:00+00:00</td>\n",
       "      <td>PartnerFTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5002</td>\n",
       "      <td>C005</td>\n",
       "      <td>275.75</td>\n",
       "      <td>2024-03-03 09:10:00+00:00</td>\n",
       "      <td>PartnerFTP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  OrderId CustomerId  NetAmount              UpdatedAtUtc SourceSystem\n",
       "0     101       C001     250.00 2024-03-01 10:15:00+00:00          AVS\n",
       "1     102       C002     190.50 2024-03-01 12:30:00+00:00          AVS\n",
       "2     103       C003     330.25 2024-03-02 09:45:00+00:00          AVS\n",
       "3   A-900       C002     210.00 2024-03-02 14:00:00+00:00     DataLake\n",
       "4   A-901       C004     480.00 2024-03-02 16:20:00+00:00     DataLake\n",
       "5    5001       C001     125.50 2024-03-03 08:30:00+00:00   PartnerFTP\n",
       "6    5002       C005     275.75 2024-03-03 09:10:00+00:00   PartnerFTP"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "warehouse_view = combined.rename(columns={\n",
    "    'OrderID': 'OrderId',\n",
    "    'CustomerID': 'CustomerId',\n",
    "    'Amount': 'NetAmount',\n",
    "    'UpdatedAt': 'UpdatedAtUtc',\n",
    "    'IngestedFrom': 'SourceSystem'\n",
    "})\n",
    "warehouse_view"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c6fbbaf",
   "metadata": {},
   "source": [
    "### 4.2 Executive Summary Layout\n",
    "\n",
    "Business stakeholders sometimes request multi-level headers that distinguish metrics from metadata. Pandas styling allows us to mimic the resulting presentation layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03a5ede3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">Identity</th>\n",
       "      <th>Financials</th>\n",
       "      <th colspan=\"2\" halign=\"left\">Operational</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>OrderId</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>NetAmount</th>\n",
       "      <th>UpdatedAtUtc</th>\n",
       "      <th>SourceSystem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>C001</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2024-03-01 10:15:00+00:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>C002</td>\n",
       "      <td>190.50</td>\n",
       "      <td>2024-03-01 12:30:00+00:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>C003</td>\n",
       "      <td>330.25</td>\n",
       "      <td>2024-03-02 09:45:00+00:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-900</td>\n",
       "      <td>C002</td>\n",
       "      <td>210.00</td>\n",
       "      <td>2024-03-02 14:00:00+00:00</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-901</td>\n",
       "      <td>C004</td>\n",
       "      <td>480.00</td>\n",
       "      <td>2024-03-02 16:20:00+00:00</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5001</td>\n",
       "      <td>C001</td>\n",
       "      <td>125.50</td>\n",
       "      <td>2024-03-03 08:30:00+00:00</td>\n",
       "      <td>PartnerFTP</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>5002</td>\n",
       "      <td>C005</td>\n",
       "      <td>275.75</td>\n",
       "      <td>2024-03-03 09:10:00+00:00</td>\n",
       "      <td>PartnerFTP</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  Identity            Financials               Operational             \n",
       "   OrderId CustomerId  NetAmount              UpdatedAtUtc SourceSystem\n",
       "0      101       C001     250.00 2024-03-01 10:15:00+00:00          AVS\n",
       "1      102       C002     190.50 2024-03-01 12:30:00+00:00          AVS\n",
       "2      103       C003     330.25 2024-03-02 09:45:00+00:00          AVS\n",
       "3    A-900       C002     210.00 2024-03-02 14:00:00+00:00     DataLake\n",
       "4    A-901       C004     480.00 2024-03-02 16:20:00+00:00     DataLake\n",
       "5     5001       C001     125.50 2024-03-03 08:30:00+00:00   PartnerFTP\n",
       "6     5002       C005     275.75 2024-03-03 09:10:00+00:00   PartnerFTP"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "executive_summary = warehouse_view.copy()\n",
    "executive_summary.columns = pd.MultiIndex.from_tuples([\n",
    "    ('Identity', 'OrderId'),\n",
    "    ('Identity', 'CustomerId'),\n",
    "    ('Financials', 'NetAmount'),\n",
    "    ('Operational', 'UpdatedAtUtc'),\n",
    "    ('Operational', 'SourceSystem')\n",
    "])\n",
    "executive_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "757f0543",
   "metadata": {},
   "source": [
    "## 5. Orchestration & Monitoring Patterns\n",
    "\n",
    "Below is a pseudo-code Airflow DAG highlighting how these transformations would be orchestrated in a production setting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9434bd93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "from airflow import DAG\n",
      "from airflow.operators.python import PythonOperator\n",
      "from datetime import datetime\n",
      "\n",
      "with DAG(\n",
      "    dag_id=\"retail_orders_pipeline\",\n",
      "    start_date=datetime(2024, 3, 1),\n",
      "    schedule_interval=\"0 * * * *\",\n",
      "    catchup=False,\n",
      ") as dag:\n",
      "\n",
      "    extract_avs = PythonOperator(\n",
      "        task_id=\"extract_avs\",\n",
      "        python_callable=extract_from_avs,\n",
      "    )\n",
      "\n",
      "    load_bronze = PythonOperator(\n",
      "        task_id=\"load_bronze\",\n",
      "        python_callable=write_to_bronze,\n",
      "    )\n",
      "\n",
      "    transform_silver = PythonOperator(\n",
      "        task_id=\"transform_silver\",\n",
      "        python_callable=standardize_headers,\n",
      "    )\n",
      "\n",
      "    publish_gold = PythonOperator(\n",
      "        task_id=\"publish_gold\",\n",
      "        python_callable=publish_to_snowflake,\n",
      "    )\n",
      "\n",
      "    extract_avs >> load_bronze >> transform_silver >> publish_gold\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from textwrap import dedent\n",
    "\n",
    "airflow_dag = dedent('''\n",
    "from airflow import DAG\n",
    "from airflow.operators.python import PythonOperator\n",
    "from datetime import datetime\n",
    "\n",
    "with DAG(\n",
    "    dag_id=\"retail_orders_pipeline\",\n",
    "    start_date=datetime(2024, 3, 1),\n",
    "    schedule_interval=\"0 * * * *\",\n",
    "    catchup=False,\n",
    ") as dag:\n",
    "\n",
    "    extract_avs = PythonOperator(\n",
    "        task_id=\"extract_avs\",\n",
    "        python_callable=extract_from_avs,\n",
    "    )\n",
    "\n",
    "    load_bronze = PythonOperator(\n",
    "        task_id=\"load_bronze\",\n",
    "        python_callable=write_to_bronze,\n",
    "    )\n",
    "\n",
    "    transform_silver = PythonOperator(\n",
    "        task_id=\"transform_silver\",\n",
    "        python_callable=standardize_headers,\n",
    "    )\n",
    "\n",
    "    publish_gold = PythonOperator(\n",
    "        task_id=\"publish_gold\",\n",
    "        python_callable=publish_to_snowflake,\n",
    "    )\n",
    "\n",
    "    extract_avs >> load_bronze >> transform_silver >> publish_gold\n",
    "''')\n",
    "print(airflow_dag)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e0a030e",
   "metadata": {},
   "source": [
    "## 6. End-to-End Demonstration\n",
    "\n",
    "Finally, we package the transformations into reusable functions to demonstrate how a single orchestration call could drive the pipeline.\n",
    "\n",
    "> **Tip:** In a real deployment, each function would live in its own module with logging, error handling, retry logic, and parameterization for environment-specific values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "83089eb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] avs_orders: headers aligned\n",
      "[OK] raw_lake_orders: headers aligned\n",
      "[OK] partner_feed: headers aligned\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(  OrderId CustomerId  NetAmount        UpdatedAtUtc SourceSystem\n",
       " 0     101       C001     250.00 2024-03-01 10:15:00          AVS\n",
       " 1     102       C002     190.50 2024-03-01 12:30:00          AVS\n",
       " 2     103       C003     330.25 2024-03-02 09:45:00          AVS\n",
       " 3   A-900       C002     210.00                 NaT     DataLake\n",
       " 4   A-901       C004     480.00                 NaT     DataLake\n",
       " 5    5001       C001     125.50                 NaT   PartnerFTP\n",
       " 6    5002       C005     275.75                 NaT   PartnerFTP,\n",
       "   Identity            Financials         Operational             \n",
       "    OrderId CustomerId  NetAmount        UpdatedAtUtc SourceSystem\n",
       " 0      101       C001     250.00 2024-03-01 10:15:00          AVS\n",
       " 1      102       C002     190.50 2024-03-01 12:30:00          AVS\n",
       " 2      103       C003     330.25 2024-03-02 09:45:00          AVS\n",
       " 3    A-900       C002     210.00                 NaT     DataLake\n",
       " 4    A-901       C004     480.00                 NaT     DataLake\n",
       " 5     5001       C001     125.50                 NaT   PartnerFTP\n",
       " 6     5002       C005     275.75                 NaT   PartnerFTP)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def standardize_source(df: pd.DataFrame, cfg_key: str) -> pd.DataFrame:\n",
    "    cfg = source_config[cfg_key]\n",
    "    validate_headers(df, source_config, source=cfg_key)\n",
    "    normalized = normalize_headers(df, cfg['rename_map'])\n",
    "    normalized['IngestedFrom'] = cfg['layer']\n",
    "    return normalized.astype({\n",
    "        'OrderID': 'string',\n",
    "        'CustomerID': 'string',\n",
    "        'Amount': 'float'\n",
    "    }, errors='ignore')\n",
    "\n",
    "def run_pipeline(sources: dict[str, pd.DataFrame]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    silver = [standardize_source(df, key) for key, df in sources.items()]\n",
    "    combined_df = pd.concat(silver, ignore_index=True)\n",
    "    combined_df['UpdatedAt'] = pd.to_datetime(combined_df['UpdatedAt'], errors='coerce')\n",
    "    combined_df = combined_df.sort_values('UpdatedAt').drop_duplicates('OrderID', keep='last')\n",
    "    warehouse_df = combined_df.rename(columns={\n",
    "        'OrderID': 'OrderId',\n",
    "        'CustomerID': 'CustomerId',\n",
    "        'Amount': 'NetAmount',\n",
    "        'UpdatedAt': 'UpdatedAtUtc',\n",
    "        'IngestedFrom': 'SourceSystem'\n",
    "    })\n",
    "    exec_summary = warehouse_df.copy()\n",
    "    exec_summary.columns = pd.MultiIndex.from_tuples([\n",
    "        ('Identity', 'OrderId'),\n",
    "        ('Identity', 'CustomerId'),\n",
    "        ('Financials', 'NetAmount'),\n",
    "        ('Operational', 'UpdatedAtUtc'),\n",
    "        ('Operational', 'SourceSystem')\n",
    "    ])\n",
    "    return warehouse_df, exec_summary\n",
    "\n",
    "warehouse_df, exec_summary = run_pipeline({\n",
    "    'avs_orders': avs_orders,\n",
    "    'raw_lake_orders': raw_lake_orders,\n",
    "    'partner_feed': partner_feed\n",
    "})\n",
    "warehouse_df, exec_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3cbfe680",
   "metadata": {},
   "source": [
    "## 7. Key Takeaways\n",
    "\n",
    "* **Schema reconciliation** is the heart of multi-source consolidation. The configuration maps give a repeatable way to translate headers, enforce types, and annotate provenance.\n",
    "* **Layered storage (Bronze ? Silver ? Gold)** enables auditable transformations and simplifies debugging when data drifts.\n",
    "* **Presentation models** can diverge dramatically from raw data structures; designing reusable formatting logic avoids one-off reporting hacks.\n",
    "\n",
    "This notebook can serve as a sandbox for experimenting with additional sources, validation rules, and monitoring hooks before wiring up production pipelines."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc970851",
   "metadata": {},
   "source": [
    "## 8. Snowflake Load (Simulated)\n",
    "\n",
    "We stage the warehouse view to a CSV and generate the Snowflake DDL/DML you would execute to load or merge the data. In practice you'd use Snowflake stages (internal or S3/Azure) and an orchestrator to run these commands.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "305efcf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Staged file: C:\\Users\\pertt\\Alleyfoo\\pipe-transformation\\simulated_storage\\staging\\orders_warehouse.csv\n",
      "\n",
      "-- Snowflake DDL\n",
      "create table if not exists ORDERS_WAREHOUSE (\n",
      "    OrderId string,\n",
      "    CustomerId string,\n",
      "    NetAmount number(18,2),\n",
      "    UpdatedAtUtc timestamp_tz,\n",
      "    SourceSystem string\n",
      ");\n",
      "\n",
      "-- Snowflake COPY\n",
      "copy into ORDERS_WAREHOUSE\n",
      "from @~/orders_warehouse.csv\n",
      "file_format = (type = csv field_optionally_enclosed_by='\"' skip_header = 1)\n",
      "on_error = 'CONTINUE';\n",
      "\n",
      "-- Snowflake MERGE\n",
      "merge into ORDERS_WAREHOUSE t\n",
      "using (select * from @~/orders_warehouse.csv (file_format => 'CSV', skip_header => 1)) s\n",
      "on t.OrderId = s.$1\n",
      "when matched then update set\n",
      "    CustomerId = s.$2,\n",
      "    NetAmount = s.$3,\n",
      "    UpdatedAtUtc = s.$4,\n",
      "    SourceSystem = s.$5\n",
      "when not matched then insert (OrderId, CustomerId, NetAmount, UpdatedAtUtc, SourceSystem)\n",
      "values (s.$1, s.$2, s.$3, s.$4, s.$5);\n"
     ]
    }
   ],
   "source": [
    "staging = BASE_PATH / 'staging'\n",
    "staging.mkdir(exist_ok=True)\n",
    "stage_file = staging / 'orders_warehouse.csv'\n",
    "warehouse_df.to_csv(stage_file, index=False)\n",
    "print('Staged file:', stage_file.resolve())\n",
    "\n",
    "pk = 'OrderId'\n",
    "create_sql = (\n",
    "\"\"\"\n",
    "create table if not exists ORDERS_WAREHOUSE (\n",
    "    OrderId string,\n",
    "    CustomerId string,\n",
    "    NetAmount number(18,2),\n",
    "    UpdatedAtUtc timestamp_tz,\n",
    "    SourceSystem string\n",
    ");\n",
    "\"\"\"\n",
    ").strip()\n",
    "\n",
    "copy_sql = (\n",
    "f\"\"\"\n",
    "copy into ORDERS_WAREHOUSE\n",
    "from @~/{stage_file.name}\n",
    "file_format = (type = csv field_optionally_enclosed_by='\"' skip_header = 1)\n",
    "on_error = 'CONTINUE';\n",
    "\"\"\"\n",
    ").strip()\n",
    "\n",
    "merge_sql = (\n",
    "f\"\"\"\n",
    "merge into ORDERS_WAREHOUSE t\n",
    "using (select * from @~/{stage_file.name} (file_format => 'CSV', skip_header => 1)) s\n",
    "on t.{pk} = s.$1\n",
    "when matched then update set\n",
    "    CustomerId = s.$2,\n",
    "    NetAmount = s.$3,\n",
    "    UpdatedAtUtc = s.$4,\n",
    "    SourceSystem = s.$5\n",
    "when not matched then insert (OrderId, CustomerId, NetAmount, UpdatedAtUtc, SourceSystem)\n",
    "values (s.$1, s.$2, s.$3, s.$4, s.$5);\n",
    "\"\"\"\n",
    ").strip()\n",
    "\n",
    "print(\"\\n-- Snowflake DDL\")\n",
    "print(create_sql)\n",
    "print(\"\\n-- Snowflake COPY\")\n",
    "print(copy_sql)\n",
    "print(\"\\n-- Snowflake MERGE\")\n",
    "print(merge_sql)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06bf2dd7",
   "metadata": {},
   "source": [
    "## 9. Header Variants & Exports\n",
    "\n",
    "Demonstrates producing multiple header conventions from the same canonical model and exporting them for downstream systems.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "191f7c8c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote:\n",
      " - C:\\Users\\pertt\\Alleyfoo\\pipe-transformation\\simulated_storage\\exports\\orders_snake_case.csv\n",
      " - C:\\Users\\pertt\\Alleyfoo\\pipe-transformation\\simulated_storage\\exports\\orders_camelCase.csv\n",
      " - C:\\Users\\pertt\\Alleyfoo\\pipe-transformation\\simulated_storage\\exports\\orders_PascalCase.csv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>order_id</th>\n",
       "      <th>customer_id</th>\n",
       "      <th>net_amount</th>\n",
       "      <th>updated_at_utc</th>\n",
       "      <th>source_system</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>C001</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2024-03-01 10:15:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>C002</td>\n",
       "      <td>190.50</td>\n",
       "      <td>2024-03-01 12:30:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>C003</td>\n",
       "      <td>330.25</td>\n",
       "      <td>2024-03-02 09:45:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-900</td>\n",
       "      <td>C002</td>\n",
       "      <td>210.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-901</td>\n",
       "      <td>C004</td>\n",
       "      <td>480.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  order_id customer_id  net_amount      updated_at_utc source_system\n",
       "0      101        C001      250.00 2024-03-01 10:15:00           AVS\n",
       "1      102        C002      190.50 2024-03-01 12:30:00           AVS\n",
       "2      103        C003      330.25 2024-03-02 09:45:00           AVS\n",
       "3    A-900        C002      210.00                 NaT      DataLake\n",
       "4    A-901        C004      480.00                 NaT      DataLake"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>orderId</th>\n",
       "      <th>customerId</th>\n",
       "      <th>netAmount</th>\n",
       "      <th>updatedAtUtc</th>\n",
       "      <th>sourceSystem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>C001</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2024-03-01 10:15:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>C002</td>\n",
       "      <td>190.50</td>\n",
       "      <td>2024-03-01 12:30:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>C003</td>\n",
       "      <td>330.25</td>\n",
       "      <td>2024-03-02 09:45:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-900</td>\n",
       "      <td>C002</td>\n",
       "      <td>210.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-901</td>\n",
       "      <td>C004</td>\n",
       "      <td>480.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  orderId customerId  netAmount        updatedAtUtc sourceSystem\n",
       "0     101       C001     250.00 2024-03-01 10:15:00          AVS\n",
       "1     102       C002     190.50 2024-03-01 12:30:00          AVS\n",
       "2     103       C003     330.25 2024-03-02 09:45:00          AVS\n",
       "3   A-900       C002     210.00                 NaT     DataLake\n",
       "4   A-901       C004     480.00                 NaT     DataLake"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderId</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>NetAmount</th>\n",
       "      <th>UpdatedAtUtc</th>\n",
       "      <th>SourceSystem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>C001</td>\n",
       "      <td>250.00</td>\n",
       "      <td>2024-03-01 10:15:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>C002</td>\n",
       "      <td>190.50</td>\n",
       "      <td>2024-03-01 12:30:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>103</td>\n",
       "      <td>C003</td>\n",
       "      <td>330.25</td>\n",
       "      <td>2024-03-02 09:45:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>A-900</td>\n",
       "      <td>C002</td>\n",
       "      <td>210.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>A-901</td>\n",
       "      <td>C004</td>\n",
       "      <td>480.00</td>\n",
       "      <td>NaT</td>\n",
       "      <td>DataLake</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  OrderId CustomerId  NetAmount        UpdatedAtUtc SourceSystem\n",
       "0     101       C001     250.00 2024-03-01 10:15:00          AVS\n",
       "1     102       C002     190.50 2024-03-01 12:30:00          AVS\n",
       "2     103       C003     330.25 2024-03-02 09:45:00          AVS\n",
       "3   A-900       C002     210.00                 NaT     DataLake\n",
       "4   A-901       C004     480.00                 NaT     DataLake"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "def to_snake(name: str) -> str:\n",
    "    s1 = re.sub(r'(.)([A-Z][a-z]+)', r'\\1_\\2', name)\n",
    "    s2 = re.sub(r'([a-z0-9])([A-Z])', r'\\1_\\2', s1)\n",
    "    return s2.lower()\n",
    "\n",
    "def to_camel_from_pascal(name: str) -> str:\n",
    "    return name[:1].lower() + name[1:] if name else name\n",
    "\n",
    "snake_df = warehouse_df.copy()\n",
    "snake_df.columns = [to_snake(c) for c in warehouse_df.columns]\n",
    "\n",
    "camel_df = warehouse_df.copy()\n",
    "camel_df.columns = [to_camel_from_pascal(c) for c in warehouse_df.columns]\n",
    "\n",
    "pascal_df = warehouse_df.copy()  # already Pascal Case\n",
    "\n",
    "exports = BASE_PATH / 'exports'\n",
    "exports.mkdir(exist_ok=True)\n",
    "snake_path = exports / 'orders_snake_case.csv'\n",
    "camel_path = exports / 'orders_camelCase.csv'\n",
    "pascal_path = exports / 'orders_PascalCase.csv'\n",
    "snake_df.to_csv(snake_path, index=False)\n",
    "camel_df.to_csv(camel_path, index=False)\n",
    "pascal_df.to_csv(pascal_path, index=False)\n",
    "\n",
    "from IPython.display import display\n",
    "print('Wrote:')\n",
    "print(' -', snake_path.resolve())\n",
    "print(' -', camel_path.resolve())\n",
    "print(' -', pascal_path.resolve())\n",
    "display(snake_df.head())\n",
    "display(camel_df.head())\n",
    "display(pascal_df.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f88cd6b4",
   "metadata": {},
   "source": [
    "## 10. Incremental Loads & Idempotent Upserts\n",
    "\n",
    "Uses a simple watermark on the update timestamp to select new/changed rows and simulates an idempotent merge into a target table.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "e487298a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Records after watermark (2024-03-02 12:00:00+00:00): 0\n",
      "Target before: 2\n",
      "Target after: 2\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OrderId</th>\n",
       "      <th>CustomerId</th>\n",
       "      <th>NetAmount</th>\n",
       "      <th>UpdatedAtUtc</th>\n",
       "      <th>SourceSystem</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>101</td>\n",
       "      <td>C001</td>\n",
       "      <td>250.0</td>\n",
       "      <td>2024-03-01 10:15:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>C002</td>\n",
       "      <td>190.5</td>\n",
       "      <td>2024-03-01 12:30:00</td>\n",
       "      <td>AVS</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  OrderId CustomerId  NetAmount        UpdatedAtUtc SourceSystem\n",
       "0     101       C001      250.0 2024-03-01 10:15:00          AVS\n",
       "1     102       C002      190.5 2024-03-01 12:30:00          AVS"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "last_watermark = pd.Timestamp('2024-03-02 12:00:00', tz='UTC')\n",
    "ts = pd.to_datetime(warehouse_df['UpdatedAtUtc'], errors='coerce', utc=True)\n",
    "incremental = warehouse_df.loc[ts > last_watermark]\n",
    "print(f'Records after watermark ({last_watermark}):', len(incremental))\n",
    "\n",
    "# Simulate an existing target snapshot (e.g., previous load)\n",
    "target_before = warehouse_df.iloc[[0, 1]].copy()\n",
    "print('Target before:', len(target_before))\n",
    "\n",
    "# Idempotent upsert: keep non-matching existing + replace with latest for matching keys\n",
    "updated = pd.concat([\n",
    "    target_before[~target_before['OrderId'].isin(incremental['OrderId'])],\n",
    "    incremental\n",
    "], ignore_index=True)\n",
    "print('Target after:', len(updated))\n",
    "updated.sort_values('OrderId').head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2848949",
   "metadata": {},
   "source": [
    "## 11. Connector Techniques (Stubs)\n",
    "\n",
    "When live credentials or SDKs are unavailable, we can still demonstrate the technique. Below are minimal templates for common connectors:\n",
    "\n",
    "- Azure Blob Storage (landing/bronze)\n",
    "- AWS S3 (landing/bronze)\n",
    "- Snowflake (warehouse)\n",
    "- SFTP/AVS (on-prem legacy feed)\n",
    "\n",
    "Install (suggested):\n",
    "\n",
    "pip install azure-storage-blob boto3 snowflake-connector-python paramiko\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "ffe25015",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Azure Blob example ---\n",
      " \n",
      "from azure.storage.blob import BlobServiceClient\n",
      "\n",
      "conn_str = \"<AZURE_STORAGE_CONNECTION_STRING>\"\n",
      "container = \"landing\"\n",
      "prefix = \"orders/\"\n",
      "\n",
      "svc = BlobServiceClient.from_connection_string(conn_str)\n",
      "container_client = svc.get_container_client(container)\n",
      "for blob in container_client.list_blobs(name_starts_with=prefix):\n",
      "    print(blob.name)\n",
      "\n",
      "\n",
      "--- S3 example ---\n",
      " \n",
      "import boto3\n",
      "\n",
      "s3 = boto3.client('s3')\n",
      "bucket = '<BUCKET_NAME>'\n",
      "prefix = 'orders/'\n",
      "\n",
      "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
      "for obj in resp.get('Contents', []):\n",
      "    print(obj['Key'])\n",
      "\n",
      "\n",
      "--- Snowflake example ---\n",
      " \n",
      "import snowflake.connector as sf\n",
      "\n",
      "conn = sf.connect(\n",
      "    user='<USER>', password='<PASSWORD>', account='<ACCOUNT>',\n",
      "    warehouse='COMPUTE_WH', database='ANALYTICS', schema='PUBLIC'\n",
      ")\n",
      "cur = conn.cursor()\n",
      "cur.execute(\"create table if not exists ORDERS_WAREHOUSE (OrderId string, ...)\")\n",
      "cur.execute(\"copy into ORDERS_WAREHOUSE from @~/<staged_file>.csv file_format=(type=csv skip_header=1)\")\n",
      "cur.close(); conn.close()\n",
      "\n",
      "\n",
      "--- SFTP/AVS example ---\n",
      " \n",
      "import paramiko\n",
      "\n",
      "host = '<SFTP_HOST>'\n",
      "user = '<USER>'\n",
      "key_path = '<PRIVATE_KEY_PATH>'\n",
      "\n",
      "key = paramiko.RSAKey.from_private_key_file(key_path)\n",
      "client = paramiko.SSHClient(); client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
      "client.connect(hostname=host, username=user, pkey=key)\n",
      "sftp = client.open_sftp()\n",
      "for name in sftp.listdir('incoming/avs'): print(name)\n",
      "sftp.close(); client.close()\n",
      "\n"
     ]
    }
   ],
   "source": [
    "azure_blob_template = \"\"\"\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "conn_str = \"<AZURE_STORAGE_CONNECTION_STRING>\"\n",
    "container = \"landing\"\n",
    "prefix = \"orders/\"\n",
    "\n",
    "svc = BlobServiceClient.from_connection_string(conn_str)\n",
    "container_client = svc.get_container_client(container)\n",
    "for blob in container_client.list_blobs(name_starts_with=prefix):\n",
    "    print(blob.name)\n",
    "\"\"\"\n",
    "\n",
    "s3_template = \"\"\"\n",
    "import boto3\n",
    "\n",
    "s3 = boto3.client('s3')\n",
    "bucket = '<BUCKET_NAME>'\n",
    "prefix = 'orders/'\n",
    "\n",
    "resp = s3.list_objects_v2(Bucket=bucket, Prefix=prefix)\n",
    "for obj in resp.get('Contents', []):\n",
    "    print(obj['Key'])\n",
    "\"\"\"\n",
    "\n",
    "snowflake_template = \"\"\"\n",
    "import snowflake.connector as sf\n",
    "\n",
    "conn = sf.connect(\n",
    "    user='<USER>', password='<PASSWORD>', account='<ACCOUNT>',\n",
    "    warehouse='COMPUTE_WH', database='ANALYTICS', schema='PUBLIC'\n",
    ")\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"create table if not exists ORDERS_WAREHOUSE (OrderId string, ...)\")\n",
    "cur.execute(\"copy into ORDERS_WAREHOUSE from @~/<staged_file>.csv file_format=(type=csv skip_header=1)\")\n",
    "cur.close(); conn.close()\n",
    "\"\"\"\n",
    "\n",
    "sftp_template = \"\"\"\n",
    "import paramiko\n",
    "\n",
    "host = '<SFTP_HOST>'\n",
    "user = '<USER>'\n",
    "key_path = '<PRIVATE_KEY_PATH>'\n",
    "\n",
    "key = paramiko.RSAKey.from_private_key_file(key_path)\n",
    "client = paramiko.SSHClient(); client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "client.connect(hostname=host, username=user, pkey=key)\n",
    "sftp = client.open_sftp()\n",
    "for name in sftp.listdir('incoming/avs'): print(name)\n",
    "sftp.close(); client.close()\n",
    "\"\"\"\n",
    "\n",
    "print(\"--- Azure Blob example ---\\n\", azure_blob_template)\n",
    "print(\"\\n--- S3 example ---\\n\", s3_template)\n",
    "print(\"\\n--- Snowflake example ---\\n\", snowflake_template)\n",
    "print(\"\\n--- SFTP/AVS example ---\\n\", sftp_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd22f16",
   "metadata": {},
   "source": [
    "## 12. Header Rename Before Merge (Hyphenated)\n",
    "\n",
    "Example: transform a source header `key_value1` into `key-value` before merging into the main database table. Note that hyphenated identifiers require quoting in SQL (for example Snowflake/ANSI SQL: \"key-value\").\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "69f9e1a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before merge (main):\n",
      "  key-value  amount\n",
      "0      A001   100.0\n",
      "1      B002   200.0\n",
      "\n",
      "Incoming (renamed):\n",
      "  key-value  amount\n",
      "0      A001   120.0\n",
      "1      B002   220.0\n",
      "2      C003   150.0\n",
      "\n",
      "After merge (upserted):\n",
      "  key-value  amount\n",
      "0      A001   120.0\n",
      "1      B002   220.0\n",
      "2      C003   150.0\n",
      "\n",
      "Merge template SQL (Snowflake):\n",
      " merge into MAIN_DB t\n",
      "using (select column1 as \"key-value\", column2 as amount from @~/upsert.csv (file_format => 'CSV', skip_header => 1)) s\n",
      "on t.\"key-value\" = s.\"key-value\"\n",
      "when matched then update set amount = s.amount\n",
      "when not matched then insert (\"key-value\", amount) values (s.\"key-value\", s.amount);\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "# Example source feed with a non-canonical header name\n",
    "kv_feed = pd.DataFrame({\n",
    "    'key_value1': ['A001', 'B002', 'C003'],\n",
    "    'amount': [120.0, 220.0, 150.0]\n",
    "})\n",
    "\n",
    "# Simulated main database snapshot (already using hyphenated header)\n",
    "main_db = pd.DataFrame({\n",
    "    'key-value': ['A001', 'B002'],\n",
    "    'amount': [100.0, 200.0]\n",
    "})\n",
    "\n",
    "# Transform rules: underscores -> hyphens, drop trailing digits\n",
    "def header_transform(name: str) -> str:\n",
    "    name = name.replace('_', '-')\n",
    "    name = re.sub(r'\\d+$', '', name)  # remove trailing digits\n",
    "    return name\n",
    "\n",
    "kv_renamed = kv_feed.rename(columns=header_transform)\n",
    "\n",
    "print('Before merge (main):')\n",
    "print(main_db)\n",
    "print('\\nIncoming (renamed):')\n",
    "print(kv_renamed)\n",
    "\n",
    "# Idempotent upsert on the hyphenated key\n",
    "key = 'key-value'\n",
    "updated = pd.concat([\n",
    "    main_db[~main_db[key].isin(kv_renamed[key])],\n",
    "    kv_renamed\n",
    "], ignore_index=True)\n",
    "\n",
    "print('\\nAfter merge (upserted):')\n",
    "print(updated.sort_values(key).reset_index(drop=True))\n",
    "\n",
    "# Example SQL to mirror this behavior in Snowflake (note quoted identifier)\n",
    "sql_merge = \"\"\"\n",
    "merge into MAIN_DB t\n",
    "using (select column1 as \"key-value\", column2 as amount from @~/upsert.csv (file_format => 'CSV', skip_header => 1)) s\n",
    "on t.\"key-value\" = s.\"key-value\"\n",
    "when matched then update set amount = s.amount\n",
    "when not matched then insert (\"key-value\", amount) values (s.\"key-value\", s.amount);\n",
    "\"\"\".strip()\n",
    "print('\\nMerge template SQL (Snowflake):\\n', sql_merge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c59758",
   "metadata": {},
   "source": [
    "## 13. Config-Driven Orchestration (Demo)\n",
    "\n",
    "Loads a YAML configuration and uses the reusable toolkit functions to normalize headers, perform a hyphenated-key upsert, and produce a Snowflake MERGE statement. This demonstrates the technique even if live connectors are unavailable.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d095352e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Config loaded: True\n",
      "Sources: ['abfs_orders_url', 's3_orders_url']\n",
      "Targets: ['snowflake']\n",
      "After upsert rows: 2\n",
      "\n",
      "MERGE SQL preview:\n",
      "merge into ORDERS_WAREHOUSE t\n",
      "using (select * from @~/orders_warehouse.csv (file_format => 'CSV', skip_header => 1)) s\n",
      "on t.OrderId = s.$1\n",
      "when matched then update set\n",
      "    CustomerId = s.$2,\n",
      "    NetAmount = s.$3,\n",
      "    UpdatedAtUtc = s.$4,\n",
      "    SourceSystem = s.$5\n",
      "when not matched then insert (OrderId, CustomerId, NetAmount, UpdatedAtUtc, SourceSystem)\n",
      "values (s.$1, s.$2, s.$3, s.$4, s.$5);\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "# Make 'src' importable\n",
    "sys.path.append('src')\n",
    "from pipeline.toolkit import (\n",
    "    load_yaml_config,\n",
    "    header_transform_underscore_to_hyphen_remove_digits as hyph,\n",
    "    idempotent_upsert_key,\n",
    "    normalize_headers,\n",
    "    snowflake_merge_sql,\n",
    ")\n",
    "\n",
    "cfg_path = Path('config/pipeline.example.yaml')\n",
    "cfg = load_yaml_config(str(cfg_path)) if cfg_path.exists() else {}\n",
    "print('Config loaded:', bool(cfg))\n",
    "if cfg:\n",
    "    print('Sources:', list(cfg.get('sources', {}).keys()))\n",
    "    print('Targets:', list(cfg.get('targets', {}).keys()))\n",
    "\n",
    "# Demo: header hyphenation before merge\n",
    "source_df = pd.DataFrame({'key_value1': ['A001','B002'], 'amount':[120.0,210.0]})\n",
    "target_df = pd.DataFrame({'key-value': ['A001'], 'amount':[100.0]})\n",
    "\n",
    "renamed_df = source_df.rename(columns=hyph)\n",
    "merged = idempotent_upsert_key(target_df, renamed_df, 'key-value')\n",
    "print('After upsert rows:', len(merged))\n",
    "renamed_df.head(), merged.sort_values('key-value').reset_index(drop=True).head()\n",
    "\n",
    "print('\\nMERGE SQL preview:')\n",
    "print(snowflake_merge_sql('ORDERS_WAREHOUSE', 'OrderId', 'orders_warehouse.csv'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e87976",
   "metadata": {},
   "source": [
    "## 14. Connection Methods & Pipes (Techniques)\n",
    "\n",
    "This section summarizes practical ways to connect to common data deposits and shows how to define ingestion “pipes.”\n",
    "Each example is safe-by-default: it will only execute if required libraries and environment variables are present; otherwise it prints the exact template code to use.\n",
    "\n",
    "Covered below:\n",
    "- Azure Blob Storage (connection string or SAS)\n",
    "- AWS S3 (default credential chain)\n",
    "- SFTP/AVS (key or password)\n",
    "- Snowflake connector (session) and Snowpipe (CREATE STAGE/PIPE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "66c97946",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MISS] Azure Blob: install package for azure.storage.blob\n",
      "[MISS] AWS S3: install package for boto3\n",
      "[MISS] SFTP: install package for paramiko\n",
      "[MISS] Snowflake: install package for snowflake.connector\n",
      "\n",
      "Environment hints:\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def _chk(name, mod):\n",
    "    try:\n",
    "        __import__(mod)\n",
    "        print(f\"[OK] {name}: library present ({mod})\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"[MISS] {name}: install package for {mod}\")\n",
    "        return False\n",
    "\n",
    "has_azure = _chk('Azure Blob', 'azure.storage.blob')\n",
    "has_boto3 = _chk('AWS S3', 'boto3')\n",
    "has_paramiko = _chk('SFTP', 'paramiko')\n",
    "has_snowflake = _chk('Snowflake', 'snowflake.connector')\n",
    "print('\\nEnvironment hints:')\n",
    "for var in ['AZURE_STORAGE_CONNECTION_STRING','AZURE_BLOB_CONTAINER','AZURE_BLOB_PREFIX',\n",
    "            'AWS_PROFILE','AWS_ACCESS_KEY_ID','S3_BUCKET','S3_PREFIX',\n",
    "            'SFTP_HOST','SFTP_USER','SFTP_KEY_PATH','SFTP_DIR',\n",
    "            'SNOWFLAKE_USER','SNOWFLAKE_PASSWORD','SNOWFLAKE_ACCOUNT','SNOWFLAKE_WAREHOUSE','SNOWFLAKE_DATABASE','SNOWFLAKE_SCHEMA']:\n",
    "    if os.environ.get(var):\n",
    "        print(' -', var, '= set')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a3e38c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Azure Blob: List blobs under prefix when available; otherwise print template\n",
    "import os\n",
    "try:\n",
    "    from azure.storage.blob import BlobServiceClient  # type: ignore\n",
    "    _has_lib = True\n",
    "except Exception:\n",
    "    _has_lib = False\n",
    "\n",
    "conn = os.environ.get('AZURE_STORAGE_CONNECTION_STRING')\n",
    "container = os.environ.get('AZURE_BLOB_CONTAINER')\n",
    "prefix = os.environ.get('AZURE_BLOB_PREFIX','')\n",
    "\n",
    "if _has_lib and conn and container:\n",
    "    try:\n",
    "        svc = BlobServiceClient.from_connection_string(conn)\n",
    "        cc = svc.get_container_client(container)\n",
    "        print(f'Listing azure blobs: container={container} prefix={prefix!r}')\n",
    "        for i, blob in enumerate(cc.list_blobs(name_starts_with=prefix)):\n",
    "            print(' -', blob.name)\n",
    "            if i >= 20:\n",
    "                print(' ... (truncated)')\n",
    "                break\n",
    "    except Exception as e:\n",
    "        print('[ERR] Azure list failed:', e)\n",
    "else:\n",
    "    azure_template = \"\"\"\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "conn_str = os.environ['AZURE_STORAGE_CONNECTION_STRING']\n",
    "container = os.environ.get('AZURE_BLOB_CONTAINER','landing')\n",
    "prefix = os.environ.get('AZURE_BLOB_PREFIX','orders/')\n",
    "svc = BlobServiceClient.from_connection_string(conn_str)\n",
    "cc = svc.get_container_client(container)\n",
    "for blob in cc.list_blobs(name_starts_with=prefix):\n",
    "    print(blob.name)\n",
    "\"\"\".strip()\n",
    "    print('Azure Blob template:\\n', azure_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "23a47698",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S3 template:\n",
      " import boto3\n",
      "s3 = boto3.client('s3')\n",
      "bucket = os.environ.get('S3_BUCKET','my-raw-bucket')\n",
      "prefix = os.environ.get('S3_PREFIX','orders/')\n",
      "for page in s3.get_paginator('list_objects_v2').paginate(Bucket=bucket, Prefix=prefix):\n",
      "    for obj in page.get('Contents', []):\n",
      "        print(obj['Key'])\n"
     ]
    }
   ],
   "source": [
    "# AWS S3: List keys under prefix when available; otherwise print template\n",
    "import os\n",
    "try:\n",
    "    import boto3  # type: ignore\n",
    "    _has_boto = True\n",
    "except Exception:\n",
    "    _has_boto = False\n",
    "\n",
    "bucket = os.environ.get('S3_BUCKET')\n",
    "prefix = os.environ.get('S3_PREFIX','')\n",
    "\n",
    "if _has_boto and bucket:\n",
    "    try:\n",
    "        s3 = boto3.client('s3')\n",
    "        print(f'Listing s3 keys: bucket={bucket} prefix={prefix!r}')\n",
    "        paginator = s3.get_paginator('list_objects_v2')\n",
    "        count = 0\n",
    "        for page in paginator.paginate(Bucket=bucket, Prefix=prefix):\n",
    "            for obj in page.get('Contents', []):\n",
    "                print(' -', obj['Key'])\n",
    "                count += 1\n",
    "                if count >= 25:\n",
    "                    print(' ... (truncated)')\n",
    "                    raise StopIteration\n",
    "    except StopIteration:\n",
    "        pass\n",
    "    except Exception as e:\n",
    "        print('[ERR] S3 list failed:', e)\n",
    "else:\n",
    "    s3_template = \"\"\"\n",
    "import boto3\n",
    "s3 = boto3.client('s3')\n",
    "bucket = os.environ.get('S3_BUCKET','my-raw-bucket')\n",
    "prefix = os.environ.get('S3_PREFIX','orders/')\n",
    "for page in s3.get_paginator('list_objects_v2').paginate(Bucket=bucket, Prefix=prefix):\n",
    "    for obj in page.get('Contents', []):\n",
    "        print(obj['Key'])\n",
    "\"\"\".strip()\n",
    "    print('S3 template:\\n', s3_template)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b53653a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SFTP (AVS-like): only prints template unless DO_SFTP_DEMO=1 is set\n",
    "import os\n",
    "_do = os.environ.get('DO_SFTP_DEMO') == '1'\n",
    "try:\n",
    "    import paramiko  # type: ignore\n",
    "    _has_paramiko = True\n",
    "except Exception:\n",
    "    _has_paramiko = False\n",
    "\n",
    "if _has_paramiko and _do:\n",
    "    host = os.environ['SFTP_HOST']\n",
    "    user = os.environ['SFTP_USER']\n",
    "    key_path = os.environ['SFTP_KEY_PATH']\n",
    "    remote_dir = os.environ.get('SFTP_DIR','/incoming')\n",
    "    key = paramiko.RSAKey.from_private_key_file(key_path)\n",
    "    client = paramiko.SSHClient(); client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "    client.connect(hostname=host, username=user, pkey=key)\n",
    "    sftp = client.open_sftp()\n",
    "    try:\n",
    "        for name in sftp.listdir(remote_dir):\n",
    "            print(' -', name)\n",
    "    finally:\n",
    "        sftp.close(); client.close()\n",
    "else:\n",
    "    sftp_template = \"\"\"\n",
    "import paramiko\n",
    "host = os.environ['SFTP_HOST']\n",
    "user = os.environ['SFTP_USER']\n",
    "key_path = os.environ['SFTP_KEY_PATH']\n",
    "remote_dir = os.environ.get('SFTP_DIR','/incoming')\n",
    "key = paramiko.RSAKey.from_private_key_file(key_path)\n",
    "client = paramiko.SSHClient(); client.set_missing_host_key_policy(paramiko.AutoAddPolicy())\n",
    "client.connect(hostname=host, username=user, pkey=key)\n",
    "sftp = client.open_sftp()\n",
    "for name in sftp.listdir(remote_dir):\n",
    "    print(name)\n",
    "sftp.close(); client.close()\n",
    "\"\"\".strip()\n",
    "    print('SFTP template:\\n', sftp_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "926f1a35",
   "metadata": {},
   "source": [
    "### Snowflake Sessions, Stages, and Pipes\n",
    "\n",
    "Below: how to connect a session, define an external stage for your object store, and create a Snowpipe (auto-ingest) or use COPY/MERGE for batch loads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "0337cfa0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "External stage (Azure) template:\n",
      " -- Azure Blob external stage (SAS or storage integration)\n",
      "create or replace stage RAW_ORDERS\n",
      "  url='azure://<account>.blob.core.windows.net/<container>/<prefix>'\n",
      "  credentials=(azure_sas_token='<SAS_TOKEN>');\n",
      "\n",
      "External stage (S3) template:\n",
      " -- S3 external stage\n",
      "create or replace stage RAW_ORDERS\n",
      "  url='s3://my-raw-bucket/orders/'\n",
      "  credentials=(aws_key_id='$AWS_ACCESS_KEY_ID' aws_secret_key='$AWS_SECRET_ACCESS_KEY');\n",
      "\n",
      "Snowpipe template:\n",
      " -- Snowpipe auto-ingest from external stage to table\n",
      "create or replace pipe LOAD_ORDERS as\n",
      "  copy into ORDERS_WAREHOUSE\n",
      "  from @RAW_ORDERS\n",
      "  file_format = (type = csv field_optionally_enclosed_by='\"' skip_header = 1)\n",
      "  on_error = 'CONTINUE';\n",
      "-- Optionally set notifications (S3 SNS / Azure Event Grid) per Snowflake docs.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "sf_user = os.environ.get('SNOWFLAKE_USER')\n",
    "sf_pwd = os.environ.get('SNOWFLAKE_PASSWORD')\n",
    "sf_account = os.environ.get('SNOWFLAKE_ACCOUNT')\n",
    "sf_wh = os.environ.get('SNOWFLAKE_WAREHOUSE','COMPUTE_WH')\n",
    "sf_db = os.environ.get('SNOWFLAKE_DATABASE','ANALYTICS')\n",
    "sf_schema = os.environ.get('SNOWFLAKE_SCHEMA','PUBLIC')\n",
    "\n",
    "try:\n",
    "    import snowflake.connector as sf\n",
    "    _has_sf = True\n",
    "except Exception:\n",
    "    _has_sf = False\n",
    "\n",
    "if _has_sf and sf_user and sf_pwd and sf_account:\n",
    "    try:\n",
    "        conn = sf.connect(user=sf_user, password=sf_pwd, account=sf_account, warehouse=sf_wh, database=sf_db, schema=sf_schema)\n",
    "        cur = conn.cursor()\n",
    "        cur.execute('select 1')\n",
    "        print('[OK] Snowflake connected; example query ran.')\n",
    "        cur.close(); conn.close()\n",
    "    except Exception as e:\n",
    "        print('[ERR] Snowflake connect failed:', e)\n",
    "\n",
    "stage_azure_template = \"\"\"\n",
    "-- Azure Blob external stage (SAS or storage integration)\n",
    "create or replace stage RAW_ORDERS\n",
    "  url='azure://<account>.blob.core.windows.net/<container>/<prefix>'\n",
    "  credentials=(azure_sas_token='<SAS_TOKEN>');\n",
    "\"\"\".strip()\n",
    "\n",
    "stage_s3_template = \"\"\"\n",
    "-- S3 external stage\n",
    "create or replace stage RAW_ORDERS\n",
    "  url='s3://my-raw-bucket/orders/'\n",
    "  credentials=(aws_key_id='$AWS_ACCESS_KEY_ID' aws_secret_key='$AWS_SECRET_ACCESS_KEY');\n",
    "\"\"\".strip()\n",
    "\n",
    "pipe_template = \"\"\"\n",
    "-- Snowpipe auto-ingest from external stage to table\n",
    "create or replace pipe LOAD_ORDERS as\n",
    "  copy into ORDERS_WAREHOUSE\n",
    "  from @RAW_ORDERS\n",
    "  file_format = (type = csv field_optionally_enclosed_by='\"' skip_header = 1)\n",
    "  on_error = 'CONTINUE';\n",
    "-- Optionally set notifications (S3 SNS / Azure Event Grid) per Snowflake docs.\n",
    "\"\"\".strip()\n",
    "\n",
    "print('\\nExternal stage (Azure) template:\\n', stage_azure_template)\n",
    "print('\\nExternal stage (S3) template:\\n', stage_s3_template)\n",
    "print('\\nSnowpipe template:\\n', pipe_template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1e704ea",
   "metadata": {},
   "source": [
    "## 15. FSSPEC Paths (abfs://, s3://) + Config\n",
    "\n",
    "Use pandas with fsspec-compatible URLs to read from object stores directly. This cell scans the YAML config for any sources with a `url` and shows how we would read them. For safety, set `FSSPEC_DEMO=1` to actually attempt reads.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "93e8b21d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "URL-based sources:\n",
      " - abfs_orders_url => abfs://<container>@<account>.dfs.core.windows.net/orders/orders.csv\n",
      " - s3_orders_url => s3://my-raw-bucket/orders/orders.csv\n",
      "\n",
      "Set FSSPEC_DEMO=1 to attempt real reads; otherwise templates are shown above.\n"
     ]
    }
   ],
   "source": [
    "import os, sys\n",
    "from pathlib import Path\n",
    "import yaml\n",
    "\n",
    "sys.path.append('src')\n",
    "from pipeline.toolkit import read_uri\n",
    "\n",
    "cfg_path = Path('config/pipeline.example.yaml')\n",
    "if not cfg_path.exists():\n",
    "    print('No config found at', cfg_path)\n",
    "else:\n",
    "    cfg = yaml.safe_load(cfg_path.read_text(encoding='utf-8'))\n",
    "    with_url = {k:v for k,v in cfg.get('sources',{}).items() if isinstance(v, dict) and 'url' in v}\n",
    "    if not with_url:\n",
    "        print('No url-based sources defined in config.')\n",
    "    else:\n",
    "        print('URL-based sources:')\n",
    "        for k, meta in with_url.items():\n",
    "            print(' -', k, '=>', meta['url'])\n",
    "\n",
    "    if os.environ.get('FSSPEC_DEMO') == '1':\n",
    "        for k, meta in with_url.items():\n",
    "            try:\n",
    "                so = meta.get('storage_options', {}) if isinstance(meta, dict) else {}\n",
    "                print(f\"Reading head from {k} ...\")\n",
    "                df = read_uri(meta['url'], storage_options=so)\n",
    "                display(df.head())\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] {k}: could not read ->\", e)\n",
    "    else:\n",
    "        print('\\nSet FSSPEC_DEMO=1 to attempt real reads; otherwise templates are shown above.')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f6f5fd",
   "metadata": {},
   "source": [
    "## 16. Snowflake write_pandas (Append and Upsert)\n",
    "\n",
    "Directly write a pandas DataFrame to Snowflake using `write_pandas`. This is convenient for demos and small/medium loads. For safety, set `DO_SNOWFLAKE_DEMO=1` to actually run; otherwise templates are printed.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "a1aa33e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "write_pandas append template:\n",
      " from snowflake.connector import connect\n",
      "from snowflake.connector.pandas_tools import write_pandas\n",
      "conn = connect(user=..., password=..., account=..., warehouse='COMPUTE_WH', database='ANALYTICS', schema='PUBLIC')\n",
      "success, nchunks, nrows, _ = write_pandas(conn, warehouse_df, table_name='ORDERS_WAREHOUSE', quote_identifiers=True, auto_create_table=True)\n",
      "conn.close()\n",
      "\n",
      "MERGE upsert template:\n",
      " -- Upsert pattern using a staging table\n",
      "merge into ORDERS_WAREHOUSE t\n",
      "using ORDERS_WAREHOUSE_LOAD s\n",
      "on t.\"OrderId\" = s.\"OrderId\"\n",
      "when matched then update set\n",
      "  \"CustomerId\" = s.\"CustomerId\",\n",
      "  \"NetAmount\" = s.\"NetAmount\",\n",
      "  \"UpdatedAtUtc\" = s.\"UpdatedAtUtc\",\n",
      "  \"SourceSystem\" = s.\"SourceSystem\"\n",
      "when not matched then insert (\"OrderId\",\"CustomerId\",\"NetAmount\",\"UpdatedAtUtc\",\"SourceSystem\")\n",
      "values (s.\"OrderId\", s.\"CustomerId\", s.\"NetAmount\", s.\"UpdatedAtUtc\", s.\"SourceSystem\");\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "try:\n",
    "    import snowflake.connector as sf\n",
    "    from snowflake.connector.pandas_tools import write_pandas\n",
    "    _has_sf = True\n",
    "except Exception:\n",
    "    _has_sf = False\n",
    "\n",
    "do_demo = os.environ.get('DO_SNOWFLAKE_DEMO') == '1'\n",
    "sf_user = os.environ.get('SNOWFLAKE_USER')\n",
    "sf_pwd = os.environ.get('SNOWFLAKE_PASSWORD')\n",
    "sf_account = os.environ.get('SNOWFLAKE_ACCOUNT')\n",
    "sf_wh = os.environ.get('SNOWFLAKE_WAREHOUSE','COMPUTE_WH')\n",
    "sf_db = os.environ.get('SNOWFLAKE_DATABASE','ANALYTICS')\n",
    "sf_schema = os.environ.get('SNOWFLAKE_SCHEMA','PUBLIC')\n",
    "table = 'ORDERS_WAREHOUSE'\n",
    "\n",
    "append_template = \"\"\"\n",
    "from snowflake.connector import connect\n",
    "from snowflake.connector.pandas_tools import write_pandas\n",
    "conn = connect(user=..., password=..., account=..., warehouse='COMPUTE_WH', database='ANALYTICS', schema='PUBLIC')\n",
    "success, nchunks, nrows, _ = write_pandas(conn, warehouse_df, table_name='ORDERS_WAREHOUSE', quote_identifiers=True, auto_create_table=True)\n",
    "conn.close()\n",
    "\"\"\".strip()\n",
    "\n",
    "merge_template = \"\"\"\n",
    "-- Upsert pattern using a staging table\n",
    "merge into ORDERS_WAREHOUSE t\n",
    "using ORDERS_WAREHOUSE_LOAD s\n",
    "on t.\"OrderId\" = s.\"OrderId\"\n",
    "when matched then update set\n",
    "  \"CustomerId\" = s.\"CustomerId\",\n",
    "  \"NetAmount\" = s.\"NetAmount\",\n",
    "  \"UpdatedAtUtc\" = s.\"UpdatedAtUtc\",\n",
    "  \"SourceSystem\" = s.\"SourceSystem\"\n",
    "when not matched then insert (\"OrderId\",\"CustomerId\",\"NetAmount\",\"UpdatedAtUtc\",\"SourceSystem\")\n",
    "values (s.\"OrderId\", s.\"CustomerId\", s.\"NetAmount\", s.\"UpdatedAtUtc\", s.\"SourceSystem\");\n",
    "\"\"\".strip()\n",
    "\n",
    "if not _has_sf or not do_demo or not (sf_user and sf_pwd and sf_account):\n",
    "    print('write_pandas append template:\\n', append_template)\n",
    "    print('\\nMERGE upsert template:\\n', merge_template)\n",
    "else:\n",
    "    conn = sf.connect(user=sf_user, password=sf_pwd, account=sf_account, warehouse=sf_wh, database=sf_db, schema=sf_schema)\n",
    "    try:\n",
    "        ok, nchunks, nrows, _ = write_pandas(conn, warehouse_df, table_name=table, quote_identifiers=True, auto_create_table=True)\n",
    "        print('Appended rows:', nrows, 'chunks:', nchunks, 'ok:', ok)\n",
    "        # Optionally demonstrate upsert via staging table\n",
    "        ok, nchunks, nrows, _ = write_pandas(conn, warehouse_df, table_name=f\"{table}_LOAD\", quote_identifiers=True, overwrite=True, auto_create_table=True)\n",
    "        cur = conn.cursor();\n",
    "        cur.execute(\"\"\"\n",
    "merge into ORDERS_WAREHOUSE t\n",
    "using ORDERS_WAREHOUSE_LOAD s\n",
    "on t.\"OrderId\" = s.\"OrderId\"\n",
    "when matched then update set\n",
    "  \"CustomerId\" = s.\"CustomerId\",\n",
    "  \"NetAmount\" = s.\"NetAmount\",\n",
    "  \"UpdatedAtUtc\" = s.\"UpdatedAtUtc\",\n",
    "  \"SourceSystem\" = s.\"SourceSystem\"\n",
    "when not matched then insert (\"OrderId\",\"CustomerId\",\"NetAmount\",\"UpdatedAtUtc\",\"SourceSystem\")\n",
    "values (s.\"OrderId\", s.\"CustomerId\", s.\"NetAmount\", s.\"UpdatedAtUtc\", s.\"SourceSystem\");\n",
    "\"\"\")\n",
    "        cur.close()\n",
    "        print('Upsert (MERGE) applied from staging table')\n",
    "    finally:\n",
    "        conn.close()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c41218b",
   "metadata": {},
   "source": [
    "## 17. Semi-Structured JSON (VARIANT) Pattern\n",
    "\n",
    "When structure is unknown or variable, land JSON into a `VARIANT` column and project only the fields you need later.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "622f4375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- JSON DDL:\n",
      " create or replace file format JSON_FF type = json;\n",
      "create table if not exists RAW_JSON (\n",
      "  v variant,\n",
      "  src string,\n",
      "  loaded_at timestamp_tz default current_timestamp()\n",
      ");\n",
      "\n",
      "-- JSON LOAD:\n",
      " -- Stage JSON files and load\n",
      "PUT file://C:/path/to/events/*.json @~/json AUTO_COMPRESS=TRUE OVERWRITE=TRUE;\n",
      "COPY INTO RAW_JSON FROM @~/json FILE_FORMAT=(FORMAT_NAME=>JSON_FF) ON_ERROR='CONTINUE';\n",
      "\n",
      "-- JSON PROJECT:\n",
      " -- Project to a typed table/view\n",
      "create or replace table CURATED_JSON as\n",
      "select\n",
      "  v:\"orderId\"::string       as \"OrderId\",\n",
      "  v:\"customer\"::string      as \"CustomerId\",\n",
      "  v:\"amount\"::number        as \"NetAmount\",\n",
      "  v:\"updatedAt\"::timestamp_tz as \"UpdatedAtUtc\"\n",
      "from RAW_JSON;\n"
     ]
    }
   ],
   "source": [
    "json_ddl = \"\"\"\n",
    "create or replace file format JSON_FF type = json;\n",
    "create table if not exists RAW_JSON (\n",
    "  v variant,\n",
    "  src string,\n",
    "  loaded_at timestamp_tz default current_timestamp()\n",
    ");\n",
    "\"\"\".strip()\n",
    "\n",
    "json_load = \"\"\"\n",
    "-- Stage JSON files and load\n",
    "PUT file://C:/path/to/events/*.json @~/json AUTO_COMPRESS=TRUE OVERWRITE=TRUE;\n",
    "COPY INTO RAW_JSON FROM @~/json FILE_FORMAT=(FORMAT_NAME=>JSON_FF) ON_ERROR='CONTINUE';\n",
    "\"\"\".strip()\n",
    "\n",
    "json_project = \"\"\"\n",
    "-- Project to a typed table/view\n",
    "create or replace table CURATED_JSON as\n",
    "select\n",
    "  v:\"orderId\"::string       as \"OrderId\",\n",
    "  v:\"customer\"::string      as \"CustomerId\",\n",
    "  v:\"amount\"::number        as \"NetAmount\",\n",
    "  v:\"updatedAt\"::timestamp_tz as \"UpdatedAtUtc\"\n",
    "from RAW_JSON;\n",
    "\"\"\".strip()\n",
    "\n",
    "print('-- JSON DDL:\\n', json_ddl)\n",
    "print('\\n-- JSON LOAD:\\n', json_load)\n",
    "print('\\n-- JSON PROJECT:\\n', json_project)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bb8ea64",
   "metadata": {},
   "source": [
    "## 18. Automation & Scheduling (Safe Templates)\n",
    "\n",
    "This section shows how to automate daily runs on Windows using Task Scheduler. It also provides templates for Airflow and GitHub Actions. These examples are safe: they only print the exact commands unless you opt in.\n",
    "\n",
    "- Daily run target (with optional Snowflake write):\n",
    "  - `python scripts/run_pipeline.py --run`\n",
    "  - `python scripts/run_pipeline.py --run --write-snowflake upsert`\n",
    "- Environment: use `.env` (see `.env.example`); the CLI auto-loads it.\n",
    "- Helper script: `scripts/scheduled_run.ps1` wraps activation, execution, and logging.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "3725043f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Task Scheduler (PowerShell) templates:\n",
      "\n",
      "1) UI setup (Action):\n",
      "  Program: powershell.exe\n",
      "  Arguments: -NoProfile -ExecutionPolicy Bypass -Command \"cd 'C:\\\\Users\\\\pertt\\\\Alleyfoo\\\\pipe-transformation'; . .\\.venv\\Scripts\\Activate.ps1; python scripts\\run_pipeline.py --run --write-snowflake upsert\"\n",
      "\n",
      "2) Register via PowerShell:\n",
      "\n",
      "$action  = New-ScheduledTaskAction -Execute 'powershell.exe' -Argument \"-NoProfile -ExecutionPolicy Bypass -Command \"cd 'C:\\\\Users\\\\pertt\\\\Alleyfoo\\\\pipe-transformation'; . .\\.venv\\Scripts\\Activate.ps1; python scripts\\run_pipeline.py --run --write-snowflake upsert\"\"\n",
      "$trigger = New-ScheduledTaskTrigger -Daily -At 2:00am\n",
      "Register-ScheduledTask -Action $action -Trigger $trigger -TaskName 'PipeTransformationDaily' -Description 'Run pipeline demo daily' -RunLevel Highest\n",
      "\n",
      "\n",
      "3) Log rotation tip (inside scheduled_run.ps1): Tee-Object to logs\\run-<timestamp>.log\n",
      "\n",
      "Airflow DAG (high-level): extract -> bronze -> silver -> warehouse\n",
      "See earlier Section 5 for an example DAG definition (pseudo-code).\n",
      "\n",
      "GitHub Actions (Windows runner) outline:\n",
      "\n",
      "name: nightly-pipeline\n",
      "on:\n",
      "  schedule:\n",
      "    - cron: '0 2 * * *'\n",
      "jobs:\n",
      "  run:\n",
      "    runs-on: windows-latest\n",
      "    steps:\n",
      "      - uses: actions/checkout@v4\n",
      "      - uses: actions/setup-python@v5\n",
      "        with: { python-version: '3.12' }\n",
      "      - run: python -m pip install -r requirements.txt\n",
      "      - run: python scripts/run_pipeline.py --run\n",
      "        env:\n",
      "          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}\n",
      "          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}\n",
      "          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}\n",
      "          SNOWFLAKE_WAREHOUSE: COMPUTE_WH\n",
      "          SNOWFLAKE_DATABASE: ANALYTICS\n",
      "          SNOWFLAKE_SCHEMA: PUBLIC\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os, textwrap\n",
    "repo = r\"C:\\\\Users\\\\pertt\\\\Alleyfoo\\\\pipe-transformation\"\n",
    "ps_cmd = \"cd '\" + repo + \"'; . .\\\\.venv\\\\Scripts\\\\Activate.ps1; python scripts\\\\run_pipeline.py --run --write-snowflake upsert\"\n",
    "print('Task Scheduler (PowerShell) templates:\\n')\n",
    "print('1) UI setup (Action):\\n  Program: powershell.exe\\n  Arguments: -NoProfile -ExecutionPolicy Bypass -Command \"' + ps_cmd + '\"')\n",
    "print('\\n2) Register via PowerShell:')\n",
    "print(textwrap.dedent(f\"\"\"\n",
    "$action  = New-ScheduledTaskAction -Execute 'powershell.exe' -Argument \"-NoProfile -ExecutionPolicy Bypass -Command \\\"{ps_cmd}\\\"\"\n",
    "$trigger = New-ScheduledTaskTrigger -Daily -At 2:00am\n",
    "Register-ScheduledTask -Action $action -Trigger $trigger -TaskName 'PipeTransformationDaily' -Description 'Run pipeline demo daily' -RunLevel Highest\n",
    "\"\"\"))\n",
    "print('\\n3) Log rotation tip (inside scheduled_run.ps1): Tee-Object to logs\\\\run-<timestamp>.log')\n",
    "\n",
    "print('\\nAirflow DAG (high-level): extract -> bronze -> silver -> warehouse')\n",
    "print('See earlier Section 5 for an example DAG definition (pseudo-code).')\n",
    "\n",
    "print('\\nGitHub Actions (Windows runner) outline:')\n",
    "print(textwrap.dedent(\"\"\"\n",
    "name: nightly-pipeline\n",
    "on:\n",
    "  schedule:\n",
    "    - cron: '0 2 * * *'\n",
    "jobs:\n",
    "  run:\n",
    "    runs-on: windows-latest\n",
    "    steps:\n",
    "      - uses: actions/checkout@v4\n",
    "      - uses: actions/setup-python@v5\n",
    "        with: { python-version: '3.12' }\n",
    "      - run: python -m pip install -r requirements.txt\n",
    "      - run: python scripts/run_pipeline.py --run\n",
    "        env:\n",
    "          SNOWFLAKE_USER: ${{ secrets.SNOWFLAKE_USER }}\n",
    "          SNOWFLAKE_PASSWORD: ${{ secrets.SNOWFLAKE_PASSWORD }}\n",
    "          SNOWFLAKE_ACCOUNT: ${{ secrets.SNOWFLAKE_ACCOUNT }}\n",
    "          SNOWFLAKE_WAREHOUSE: COMPUTE_WH\n",
    "          SNOWFLAKE_DATABASE: ANALYTICS\n",
    "          SNOWFLAKE_SCHEMA: PUBLIC\n",
    "\"\"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f4cb5e1-40e4-4aab-af0e-0524d3f73ad2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python_3.11_pipe_transformation_venv",
   "language": "python",
   "name": "pipe-transformation-venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
